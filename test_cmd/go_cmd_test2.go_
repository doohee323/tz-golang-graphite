package main

import (
	"fmt"
	"log"
	"net/http"
	"io/ioutil"
	"runtime"
	"sync"
	"strconv"
	"testing"
)

// 팔로잉 결괏값을 저장할 구조체
type FollowingResult struct {
	url     int
	content string
}

// 중복 URL을 처리할 구조체
type FetchedUrl struct {
	m map[int]error
	sync.Mutex
}

//  중복 저장소 이름을 처리할 구조체
type FetchedRepo struct {
	m map[int]struct{}
	sync.Mutex
}

// Crawler 인터페이스 정의
type Crawler interface {
	Crawl()
}

// 팔로잉 수집 구조체 정의
type GitHubFollowing struct {
	fetchedUrl *FetchedUrl          // 중복 URL을 처리할 멤버
	p          *Pipeline            // 파이프라인에 작업 요청을 보낼 멤버
	result     chan FollowingResult // 결괏값을 보낼 멤버
	url        int
	urls       [3]int
}

func fetch(url int) ([]byte, error) {
	var urlstr = "http://core.local.xdn.com/cm/app/monitoring/home/uptime_list?hc_id=" + strconv.Itoa(url)
	res, err := http.Get(urlstr)
	if err != nil {
		log.Println(err)
		return nil, err
	}
	defer res.Body.Close()
	doc, err := ioutil.ReadAll(res.Body)
	return doc, nil
}

func (g *GitHubFollowing) Request(url int) {
	g.p.crawler <- &GitHubFollowing{ // 구조체를 생성하여 파이프라인의 crawler 채널에 보냄
		fetchedUrl: g.fetchedUrl, // 현재 인스턴스에서 포인터를 가져와서 다시 사용
		p:          g.p,          // 현재 인스턴스에서 포인터를 가져와서 다시 사용
		result:     g.result,     // 현재 인스턴스에서 포인터를 가져와서 다시 사용
		url:        url,          // url만 새로운 값
	}
}

func (g *GitHubFollowing) Crawl() {
	g.fetchedUrl.Lock()
	if g.url == 0 {
		g.url = g.urls[0]
	} else {
		exist := false
		// 맵은 뮤텍스로 보호
		for _, b := range g.urls {
			if _, ok := g.fetchedUrl.m[b]; !ok { // URL 중복 처리 여부를 검사
				g.url = b
				exist = true
			}
		}
		if exist == false {
			g.fetchedUrl.Unlock()
			return
		}
	}
	g.fetchedUrl.Unlock()

	doc, err := fetch(g.url) // URL에서 파싱된 HTML 데이터를 가져옴
	if err != nil {          // URL을 가져오지 못했을 때
		go func(u int) { // 교착 상태가 되지 않도록 고루틴을 생성
			g.Request(u) // 파이프라인에 URL을 보냄
		}(g.url)
		return
	}

	g.fetchedUrl.Lock()
	g.fetchedUrl.m[g.url] = err // 가져온 URL은 맵에 URL과 에러 값 저장
	g.fetchedUrl.Unlock()

	g.result <- FollowingResult{g.url, string(doc)} // 사용자 이름과 URL을 팔로잉 결과 채널에 보냄
}

// Crawler 인터페이스를 처리할 파이프라인 구조체 정의
type Pipeline struct {
	crawler chan Crawler // Crawler 타입으로 채널 선언
	done    chan struct{}
	wg      *sync.WaitGroup
}

func NewPipeline() *Pipeline {
	return &Pipeline{ // 새 파이프라인 생성
		crawler: make(chan Crawler),
		done:    make(chan struct{}),
		wg:      new(sync.WaitGroup),
	}
}

// 실제 작업을 처리하는 worker 함수
func (p *Pipeline) Worker() {
	for r := range p.crawler { // crawler 채널에서 Crawler 인터페이스를 가져옴
		select {
		case <-p.done: // 채널이 닫히면 worker 함수를 빠져나옴
			return
		default:
			r.Crawl() // Crawler 인터페이스의 Crawl 함수 실행
		}
	}
}

func (p *Pipeline) Run() {
	const numWorkers = 3
	p.wg.Add(numWorkers)
	for i := 0; i < numWorkers; i++ { // 작업을 처리할 고루틴 10개 생성
		go func() {
			p.Worker()
			p.wg.Done()
		}()
	}

	go func() {
		p.wg.Wait() // 작업 고루틴이 끝날 때까지 대기
	}()
}

func TestMap(t *testing.T) {
	numCPUs := runtime.NumCPU()
	runtime.GOMAXPROCS(numCPUs) // 모든 CPU를 사용하도록 설정

	p := NewPipeline() // 파이프라인 인스턴스 생성
	p.Run()            // worker 함수를 고루틴으로 생성

	following := &GitHubFollowing{ // 팔로잉 구조체 인스턴스 생성
		fetchedUrl: &FetchedUrl{m: make(map[int]error)}, // 중복 URL 처리 맵
		p:          p,                                   // 파이프라인 인스턴스
		result:     make(chan FollowingResult),          // 팔로잉 결괏값 채널
		urls:       [3]int{1, 2, 3},
	}

	// http://core.local.xdn.com/cm/app/monitoring/home/uptime_list?hc_id=3643
	///cm/app/monitoring/home/uptime_list?hc_id=1418
	///cm/app/monitoring/home/uptime_list?hc_id=1419
	///cm/app/monitoring/home/uptime_list?hc_id=2502
	///cm/app/monitoring/home/uptime_list?hc_id=2694
	///cm/app/monitoring/home/uptime_list?hc_id=2932
	///cm/app/monitoring/home/uptime_list?hc_id=2933
	///cm/app/monitoring/home/uptime_list?hc_id=2695

	p.crawler <- following // 파이프라인에 구조체 인스턴스를 보내서 HTML 처리 작업 시작

	for i := 0; i < 3; i++ {
		select { // 채널에 결괏값이 들어왔을 때마다 값을 출력
		case f := <-following.result:
			fmt.Println(f.content)
		}
	}
}
